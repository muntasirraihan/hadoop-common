# Experiment framework

A set of tools for creating and running **experiments**.

## Preliminary concepts

* deadline model: jobs may have a *relative deadline*, which is an offset computed as (runtime of job by itself) * (1 + epsilon), where epsilon is part of the job.
* size model: most of the job parameters are fixed as global constants; the only varied size parameter is the mapRatio, which controls how much shuffled data is generated by the map stage.
* Various quantities must be estimated; this notion is captured by tracking a mean and a standard deviation rather than use a single quantity. To increase confidence (lower standard deviation) an identical measurement can be repeated.

* A *job* (sometimes *app*) is similar to the Hadoop notion, except that it is synthetically generated. A job is characterized by only its size, as explained above, and epsilon, which controls its relative deadline.
* A *run* is a pair of jobs that are run consecutively. These are specified as a low-deadline job and a high-deadline job, along with a *delta*, which is the time difference in submit times of the jobs. By convention the first job (job0) is the low-deadline job and delta > 0 means the jobs are submitted with job0 first.
* An *experiment*, finally, is a set of independent runs intended to explore how varying some parameter affects the results.
* The results of an experiment are a number *s* defined as the percentage of jobs that met their deadlines (with two jobs, this is one of 0, 0.5 or 1). Another result is the *margin* with which each job completed; margin is deadline - finishTime, so that margin > 0 means the job finished (margin) early while margin < 0 means the job missed its deadline by (-margin).

A few quick notes about python things: `experiment.py` is a module, which is imported by multiple scripts. `logger.py` functions as both a script and a module; the code under `if __name__ == "__main__"` is executed only if called as a script. `pickle` is the Python marshalling format, which can serialize arbitrary Python classes.



## Configuration

Configuration is all JSON based. There is a `global.json` which has a few global constants. These include a couple directories that are part of the `env` hash of `ctl.py`: `common` for the location of `hadoop-common` and `target` for the extracted hadoop distribution.

## Running

Briefly, `exp.json` -> [`gen.py`] -> `<name>.pickle` -> [`estimator.py`] -> `<name>-est.pickle` -> [`runner.py`] -> `<name>-est-results.pickle`.

`exp.json` configures all the experiments, containing parameters defined on a per-experiment basis. Currently there are delta and beta experiments, where delta varies the delta parameter and beta varies the ratio of the job sizes.

`gen.py` translates the specification in `exp.json` into an experiment with unestimated jobs -- the true runtimes of the contained jobs are unknown. This is the first `.pickle` file.

`estimator.py` takes the jobs in a pickled experiment (say, `delta.pickle`) and replaces them with estimated jobs. The runtime estimate is cached in cache/runtime_estimates.pickle so that between runs an estimate can be re-used if the job size is the same. Make sure to remove this cache if the configuration changes in a way that would affect performance (like changing the node types or number of nodes in the cluster).